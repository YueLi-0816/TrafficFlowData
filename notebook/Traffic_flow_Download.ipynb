{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import collections\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. List the URL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the time every 15 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_time_string_list(start,end,interval):\n",
    "    start = datetime.strptime(start, \"%Y-%m-%d-%H-%M-%S\")\n",
    "    end = datetime.strptime(end, \"%Y-%m-%d-%H-%M-%S\")\n",
    "    now = start\n",
    "    string_list = []\n",
    "    while now <= end:\n",
    "        now += timedelta(minutes=15)\n",
    "        string_list.append(now.strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "    return string_list\n",
    "\n",
    "#time_list = get_time_string_list(\"2019-08-04-23-45-00\",\"2019-12-08-23-45-00\",15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL of the data among all sites every 15 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_list_all_data(time_list):\n",
    "    url_list = []\n",
    "    for i in range(len(time_list)):\n",
    "        if i < len(time_list) - 1:\n",
    "            url_time_start = time_list[i].split('-')\n",
    "            url_day_start = url_time_start[0] + '-' + url_time_start[1] + '-' + url_time_start[2] + 'T'\n",
    "            url_second_start = url_time_start[3] + '%3A' + url_time_start[4] + '%3A00Z'\n",
    "\n",
    "            url_time_end = time_list[i+1].split('-')\n",
    "            url_day_end = url_time_end[0] + '-' + url_time_end[1] + '-' + url_time_end[2] + 'T'\n",
    "            url_second_end = url_time_end[3] + '%3A' + url_time_end[4] + '%3A00Z'    \n",
    "\n",
    "            url_start = '&start=' + url_day_start + url_second_start\n",
    "            url_end = '&end=' + url_day_end + url_second_end\n",
    "            url = 'https://gcc.azure-api.net/traffic/v1/movement/history?size=9999' + url_start + url_end\n",
    "\n",
    "            #print(url)\n",
    "            url_list.append(url)\n",
    "\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL of every site from 2019-10-01 to 2023-09-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_list_whole_period(sites_list):\n",
    "    url_list_all_sites = []\n",
    "    for site in sites_list:\n",
    "        url = 'https://gcc.azure-api.net/traffic/v1/movement/history?size=90000&site=' + site + '&start=2019-10-01T00%3A00%3A00Z&end=2023-10-01T00%3A00%3A00Z'\n",
    "        url_list_all_sites.append(url)\n",
    "\n",
    "    return url_list_all_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get data from URL - 1033 sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reconstruct the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_data(data):\n",
    "    all_point_list = []\n",
    "    for index in range(len(data)):\n",
    "        #print(data[index])\n",
    "        each_point = {}\n",
    "        for feature, feature_value in data[index].items():\n",
    "            #print({feature: feature_value})\n",
    "            if feature != 'site':\n",
    "                feature_dict = {feature: feature_value}\n",
    "                #print(feature_dict)\n",
    "                each_point = {**each_point, **feature_dict}\n",
    "            else:\n",
    "                for site_feature, site_feature_value in data[index][feature].items():\n",
    "                    #print({site_feature: site_feature_value})\n",
    "                    if site_feature == 'from':\n",
    "                        for from_feature, from_feature_value in data[index][feature]['from'].items():\n",
    "                            from_feature_dict = {'origin' + from_feature.capitalize(): from_feature_value}\n",
    "                            #print(from_feature_dict)\n",
    "                            each_point = {**each_point, **from_feature_dict}\n",
    "                    elif site_feature == 'to':\n",
    "                        for to_feature, to_feature_value in data[index][feature]['to'].items():\n",
    "                            to_feature_dict = {'destination' + to_feature.capitalize(): to_feature_value}\n",
    "                            #print(to_feature_dict)\n",
    "                            each_point = {**each_point, **to_feature_dict}\n",
    "                    else:\n",
    "                        site_feature_dict = {'site' + site_feature.capitalize(): site_feature_value}\n",
    "                        #print(site_feature_dict)\n",
    "                        each_point = {**each_point, **site_feature_dict}\n",
    "                    #if site_feature != 'from' and site_feature != 'to':\n",
    "                        #print({'site' + site_feature.capitalize(): site_feature_value})\n",
    "        #print(each_point)\n",
    "        all_point_list.append(each_point)\n",
    "\n",
    "    all_point_dict = {}\n",
    "    for feature in all_point_list[0]:\n",
    "        all_point_dict[feature] = [each_point[feature] for each_point in all_point_list]\n",
    "    \n",
    "    all_point_df = pd.DataFrame.from_dict(all_point_dict)\n",
    "    return all_point_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean the data - 974 sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data via coords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean_coords(all_point_df):\n",
    "    #clean and rearrange df\n",
    "    simple_df = all_point_df.drop(columns = ['siteType', 'siteLastupdate', 'siteId', 'site_id', '_id'])\n",
    "    new_cols = ['type', 'lastUpdate', 'timestamp', 'batchIdentifier', 'siteSiteid', 'id', 'originDescription', 'originLat', 'originLong', 'destinationDescription', 'destinationLat', 'destinationLong', 'flow', 'concentration']\n",
    "    output_df = simple_df[new_cols]\n",
    "\n",
    "    #drop the data with zero coords\n",
    "    data_error = []\n",
    "    data_error_index = []\n",
    "    for i in range(output_df.shape[0]):\n",
    "        if output_df['originLat'][i] == '0':\n",
    "            ola_olo_dla_dlo = str(i) + '-' + output_df['originLat'][i] + '-' + output_df['originLong'][i] + '-' + output_df['destinationLat'][i] + '-' + output_df['destinationLong'][i]\n",
    "            data_error.append(ola_olo_dla_dlo)\n",
    "            data_error_index.append(i)\n",
    "        elif output_df['originLong'][i] == '0':\n",
    "            ola_olo_dla_dlo = str(i) + '-' + output_df['originLat'][i] + '-' + output_df['originLong'][i] + '-' + output_df['destinationLat'][i] + '-' + output_df['destinationLong'][i]\n",
    "            data_error.append(ola_olo_dla_dlo)\n",
    "            data_error_index.append(i)\n",
    "        elif output_df['destinationLat'][i] == '0':\n",
    "            ola_olo_dla_dlo = str(i) + '-' + output_df['originLat'][i] + '-' + output_df['originLong'][i] + '-' + output_df['destinationLat'][i] + '-' + output_df['destinationLong'][i]\n",
    "            data_error.append(ola_olo_dla_dlo)\n",
    "            data_error_index.append(i)\n",
    "        elif output_df['destinationLong'][i] == '0':\n",
    "            ola_olo_dla_dlo = str(i) + '-' + output_df['originLat'][i] + '-' + output_df['originLong'][i] + '-' + output_df['destinationLat'][i] + '-' + output_df['destinationLong'][i]\n",
    "            data_error.append(ola_olo_dla_dlo)\n",
    "            data_error_index.append(i)\n",
    "\n",
    "    output_df_correct = output_df.drop(data_error_index)\n",
    "    output_df_correct = output_df_correct.reset_index(drop=True)\n",
    "\n",
    "    #drop the data with incorrect coords\n",
    "    data_incorrect_index = []\n",
    "    data_incorrect = []\n",
    "    for i in range(output_df_correct.shape[0]):\n",
    "        if output_df_correct['originLat'][i] < '55':\n",
    "            ola_dla = str(i) + '-' + output_df_correct['originLat'][i] + '-' + output_df_correct['destinationLat'][i]\n",
    "            data_incorrect_index.append(i)\n",
    "            data_incorrect.append(ola_dla)\n",
    "        elif output_df_correct['destinationLat'][i] < '55':\n",
    "            ola_dla = str(i) + '-' + output_df_correct['originLat'][i] + '-' + output_df_correct['destinationLat'][i]\n",
    "            data_incorrect_index.append(i)\n",
    "            data_incorrect.append(ola_dla)\n",
    "\n",
    "    output_df_perfect = output_df_correct.drop(data_incorrect_index)\n",
    "    output_df_perfect = output_df_perfect.reset_index(drop=True)\n",
    "    \n",
    "    return output_df_perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data via timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_unique_time(all_time_data):\n",
    "    unique_time_dict = {}\n",
    "    all_time_list = all_time_data['timestamp']\n",
    "    date_counter = dict(collections.Counter(all_time_list))\n",
    "\n",
    "    time_list = []\n",
    "    count_list = []\n",
    "    for time, count in date_counter.items():\n",
    "        time_list.append(time)\n",
    "        count_list.append(count)\n",
    "    unique_time_dict['timestamp'] = time_list\n",
    "    unique_time_dict['count'] = count_list\n",
    "\n",
    "    unique_time_df = pd.DataFrame.from_dict(unique_time_dict)\n",
    "\n",
    "    #unique the all_time_data dataframe via timestamp\n",
    "    index = list(all_time_data.timestamp.drop_duplicates().index)\n",
    "    unique_time_data = all_time_data.iloc[index]\n",
    "    perfect_time_df = unique_time_data.merge(unique_time_df, how='left', left_on='timestamp', right_on='timestamp')\n",
    "\n",
    "    #clean and rearrange df\n",
    "    simple_df = perfect_time_df.drop(columns = ['siteType', 'siteLastupdate', 'siteId', 'site_id', '_id'])\n",
    "    new_cols = ['lastUpdate', 'timestamp', 'siteSiteid', 'count', 'originDescription', 'originLat', 'originLong', 'destinationDescription', 'destinationLat', 'destinationLong', 'flow', 'concentration', 'batchIdentifier', 'id', 'type']\n",
    "    output_perfect_df = simple_df[new_cols]\n",
    "\n",
    "    return output_perfect_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert coords WGS84 to OSGB36 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convertbng.util import convert_bng\n",
    "\n",
    "def convert_coords(great_time_df):\n",
    "    ola = [float(i) for i in list(great_time_df['originLat'])]\n",
    "    olo = [float(i) for i in list(great_time_df['originLong'])]\n",
    "    dla = [float(i) for i in list(great_time_df['destinationLat'])]\n",
    "    dlo = [float(i) for i in list(great_time_df['destinationLong'])]\n",
    "\n",
    "    o_OSGB36 = convert_bng(olo, ola)\n",
    "    d_OSGB36 = convert_bng(dlo, dla)\n",
    "    o_easting = o_OSGB36[0]\n",
    "    o_northing = o_OSGB36[1]\n",
    "    d_easting = d_OSGB36[0]\n",
    "    d_northing = d_OSGB36[1]\n",
    "\n",
    "    great_time_df['originEasting'] = o_easting\n",
    "    great_time_df['originNorthing'] = o_northing\n",
    "    great_time_df['destinationEasting'] = d_easting\n",
    "    great_time_df['destinationNorthing'] = d_northing\n",
    "\n",
    "    return great_time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the list of recorded sites\n",
    "Download one interval data to get all the sites with correct coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1033\n"
     ]
    }
   ],
   "source": [
    "time_list = get_time_string_list(\"2021-12-10-06-00-00\",\"2021-12-10-06-15-00\",15)\n",
    "url_list = get_url_list_all_data(time_list)\n",
    "for url in url_list:\n",
    "    data = get_data(url)\n",
    "    #print(len(data))\n",
    "    all_point_df = construct_data(data)\n",
    "    all_sites_df = data_clean_coords(all_point_df)\n",
    "all_sites_list = list(all_sites_df['siteSiteid'])\n",
    "print(len(set(all_sites_list)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download whole research period data with correct sites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list_whole_period = get_url_list_whole_period(all_sites_list)\n",
    "for url in url_list_whole_period:\n",
    "    data = get_data(url)\n",
    "    all_time_data = construct_data(data)\n",
    "    great_time_df = data_unique_time(all_time_data)\n",
    "    output_df = convert_coords(great_time_df)\n",
    "    \n",
    "    csv_name = url.split('&', 2)[1].split('=')[1]\n",
    "    output_df.to_csv('data/' + csv_name + '&2019_10_01-2023_09_30.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
