{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.5\n",
      "IPython version      : 8.15.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate the statistic of each record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read traffic flow data of each site (sensor) - 974 sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(filename):\n",
    "    site_df = pd.read_csv(filename)\n",
    "    \n",
    "    return site_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_2(site_df):\n",
    "    site_dup = site_df.loc[site_df['count'] > 1]\n",
    "    site_dup['startDate'] = min(site_df['lastUpdate'])\n",
    "    site_dup['errorPercentage(%)'] = (site_dup['count'].sum() - len(site_dup))/site_df['count'].sum() * 100\n",
    "    site_dup['allCount'] = site_df['count'].sum()\n",
    "       \n",
    "    return site_dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the essential information from raw traffic flow data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_clean(site_dup):\n",
    "    #clean and rearrange df\n",
    "    new_cols = ['siteSiteid', 'timestamp', 'count', 'errorPercentage(%)', 'startDate']\n",
    "    site_clean = site_dup[new_cols]\n",
    "\n",
    "    #extract a date column from timestamp\n",
    "    timestamp = site_clean['timestamp'].str.split('T', n = 1, expand = True)\n",
    "    site_clean['date'] = timestamp[0]\n",
    "    site_clean['time'] = timestamp[1]\n",
    "    \n",
    "    timestamp_raw = site_df['timestamp'].str.split('T', n = 1, expand = True)\n",
    "    site_df['date'] = timestamp_raw[0]\n",
    "    #display(site_df)\n",
    "    \n",
    "    return site_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of consecutive interpolation each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_each_day(site_clean):\n",
    "    #set date & add count\n",
    "    dates = list(set(list(site_clean['date'])))\n",
    "    #print(len(dates))\n",
    "\n",
    "    site_dateCount = {}\n",
    "    for a in dates:\n",
    "        b = site_clean.loc[site_clean.date == a , 'count']\n",
    "        c = site_df.loc[site_df.date == a, 'count']\n",
    "        site_dateCount[a] = [sum(b), sum(c)]\n",
    "    #print(site_dateCount)\n",
    "\n",
    "    dic_site_dateCount = {}\n",
    "    date = []\n",
    "    dateCount = []\n",
    "    dateCountAll = []\n",
    "    for key, value in site_dateCount.items():\n",
    "        date.append(key)\n",
    "        dateCount.append(value[0])\n",
    "        dateCountAll.append(value[1])\n",
    "        \n",
    "\n",
    "    dic_site_dateCount['errorDate'] = date\n",
    "    dic_site_dateCount['reCount/Day'] = dateCount\n",
    "    dic_site_dateCount['allCount/Day'] = dateCountAll\n",
    "\n",
    "    df_site_dateCount = pd.DataFrame.from_dict(dic_site_dateCount)\n",
    "    df_site_dateCount = df_site_dateCount.sort_values(by=['errorDate'])\n",
    "    \n",
    "    return df_site_dateCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the time of interpolation each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alltime_each_day(site_clean):\n",
    "    #set date & merge time\n",
    "    dates = list(set(list(site_clean['date'])))\n",
    "    #print(len(dates))\n",
    "\n",
    "    site_time = {}\n",
    "    for a in dates:\n",
    "        b = site_clean.loc[site_clean.date == a , 'time']\n",
    "        site_time[a] = sorted(list(set(list(b))))\n",
    "    #print(site_time)\n",
    "\n",
    "    dic_site_time = {}\n",
    "    date = []\n",
    "    time = []\n",
    "    for key, value in site_time.items():\n",
    "        date.append(key)\n",
    "        time.append(value)\n",
    "\n",
    "    dic_site_time['errorDate'] = date\n",
    "    dic_site_time['reTime/Day'] = time\n",
    "\n",
    "    df_site_time = pd.DataFrame.from_dict(dic_site_time)\n",
    "    df_site_time = df_site_time.sort_values(by=['errorDate'])\n",
    "    \n",
    "    return df_site_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the two columns together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_to_each_day(df_site_dateCount, df_site_time):\n",
    "    site_date_time = pd.merge(df_site_dateCount, df_site_time, how=\"left\", on=[\"errorDate\"])\n",
    "    site_date_time['siteId'] = site_clean['siteSiteid'].tolist()[0]\n",
    "    site_date_time['errorPercentage(%)'] = site_dup['errorPercentage(%)'].tolist()[0]\n",
    "    site_date_time['startDate'] = site_dup['startDate'].tolist()[0]\n",
    "    site_date_time['errorPercentage/Day(%)'] = (site_date_time['reCount/Day'] - 1) / site_date_time['allCount/Day'] * 100\n",
    "    site_date_time['allCount'] = site_dup['allCount'].tolist()[0]\n",
    "    new_cols = ['siteId', 'errorDate', 'reCount/Day', 'reTime/Day', 'errorPercentage/Day(%)', 'errorPercentage(%)', 'allCount', 'startDate']\n",
    "    site_date_time = site_date_time[new_cols]\n",
    "    \n",
    "    return site_date_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge each site in a single raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_to_each_site(site_date_time):\n",
    "    # columns: siteId, errorPercentage, countAll, startDate\n",
    "    site_id = site_date_time['siteId'].tolist()[0]\n",
    "    error_percentage = site_date_time['errorPercentage(%)'].tolist()[0]\n",
    "    count_all = site_date_time['allCount'].tolist()[0]\n",
    "    start_date = site_date_time['startDate'].tolist()[0]\n",
    "    \n",
    "    data = [[site_id, error_percentage, count_all, start_date]]\n",
    "    \n",
    "    # Create the pandas DataFrame\n",
    "    each_site = pd.DataFrame(data, columns = ['siteId', 'errorPercentage(%)', 'countAll', 'startDate'])\n",
    "    \n",
    "    return each_site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.fsencode('data')\n",
    "\n",
    "all_sites = []\n",
    "all_sites_date = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    \n",
    "    if filename.endswith(\".csv\"):\n",
    "        \n",
    "        site_df = read_df(filename)\n",
    "        site_dup = read_df_2(site_df)\n",
    "        site_clean = df_clean(site_dup)\n",
    "        df_site_dateCount = get_count_each_day(site_clean)\n",
    "        df_site_time = get_alltime_each_day(site_clean)\n",
    "        site_date_time = merge_to_each_day(df_site_dateCount, df_site_time)\n",
    "      \n",
    "        each_site = merge_to_each_site(site_date_time)\n",
    "\n",
    "        all_sites.append(each_site)\n",
    "        all_sites_date.append(site_date_time)\n",
    "\n",
    "\n",
    "all_statistic = pd.concat(all_sites)\n",
    "all_statistic = all_statistic.reset_index(drop=True)\n",
    "\n",
    "all_sites_day_statistic = pd.concat(all_sites_date)\n",
    "all_sites_day_statistic = all_sites_day_statistic.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean the records via statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = list(set(list(all_statistic['countAll'])))\n",
    "#print(len(counts))\n",
    "\n",
    "counts_sta = {}\n",
    "for a in counts:\n",
    "    b = all_statistic.loc[all_statistic.countAll == a , 'countAll']\n",
    "    c = all_statistic.loc[all_statistic.countAll == a , 'siteId']\n",
    "    counts_sta[a] = [len(b), list(c)]\n",
    "\n",
    "collections.OrderedDict(sorted(counts_sta.items()))\n",
    "\n",
    "dic_counts = {}\n",
    "counts = []\n",
    "countNumber = []\n",
    "sites = []\n",
    "for key, value in counts_sta.items():\n",
    "    counts.append(key)\n",
    "    countNumber.append(value[0])\n",
    "    sites.append(value[1])\n",
    "\n",
    "\n",
    "dic_counts['countAll'] = counts\n",
    "dic_counts['countNum'] = countNumber\n",
    "dic_counts['sites'] = sites\n",
    "\n",
    "df_counts = pd.DataFrame.from_dict(dic_counts)\n",
    "df_counts = df_counts.sort_values(by=['countAll'])\n",
    "df_counts = df_counts.reset_index(drop=True)\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips 1: Calculate the correct number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140256\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "d0 = date(2019, 10, 1)\n",
    "d1 = date(2023, 10, 1)\n",
    "delta = d1 - d0\n",
    "print(delta.days*24*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get start date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(set(list(all_statistic['startDate'])))\n",
    "#print(len(counts))\n",
    "\n",
    "counts_sta = {}\n",
    "for a in dates:\n",
    "    b = all_statistic.loc[all_statistic.startDate == a , 'startDate']\n",
    "    c = all_statistic.loc[all_statistic.startDate == a , 'siteId']\n",
    "    counts_sta[a] = [len(b), list(c)]\n",
    "\n",
    "#collections.OrderedDict(sorted(counts_sta.items()))\n",
    "\n",
    "dic_dates = {}\n",
    "dates = []\n",
    "dateNumber = []\n",
    "sites = []\n",
    "for key, value in counts_sta.items():\n",
    "    dates.append(key)\n",
    "    dateNumber.append(value[0])\n",
    "    sites.append(value[1])\n",
    "\n",
    "\n",
    "dic_dates['startDate'] = dates\n",
    "dic_dates['dateNumber'] = dateNumber\n",
    "dic_dates['sites'] = sites\n",
    "\n",
    "df_dates = pd.DataFrame.from_dict(dic_dates)\n",
    "df_dates = df_dates.sort_values(by=['startDate'])\n",
    "df_dates = df_dates.reset_index(drop=True)\n",
    "\n",
    "df_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the error percentage (interpolation percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error = all_statistic.loc[all_statistic['errorPercentage(%)'] > 1].iloc[:,:2]\n",
    "df_error.sort_values(by=['errorPercentage(%)'], ascending=False).reset_index(drop=True).loc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the error sites based on above statistic: \n",
    "    countsAll < 140000\n",
    "    startDate != 2019-10-01\n",
    "    errorPercentage > 3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_error = df_counts.loc[df_counts.countAll < 140000, 'sites'].tolist()\n",
    "date_error = df_dates.loc[df_dates.startDate != '2019-10-01T00:00:00.4458636Z' , 'sites'].tolist()\n",
    "error_error = df_error.loc[df_error['errorPercentage(%)'] > 3 , 'siteId'].tolist()\n",
    "\n",
    "error_list = []\n",
    "for i in range(len(count_error)):\n",
    "    error_list = error_list + count_error[i]\n",
    "for j in range(len(date_error)):\n",
    "    error_list = error_list + date_error[j]\n",
    "error_list = error_list + error_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_sites = list(set(error_list))\n",
    "error_sites_df = pd.DataFrame({'errorSites':error_sites})\n",
    "error_sites_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the error sites - 922 sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_index = []\n",
    "for error_site in error_sites_df.errorSites.tolist():\n",
    "    #print(error_site)\n",
    "    all_error_sites = raw_df.loc[raw_df['siteId'] == error_site]\n",
    "    error_index = error_index + all_error_sites.index.tolist()\n",
    "\n",
    "all_useful_sites = all_statistic.drop(error_index)\n",
    "all_useful_sites = all_useful_sites.sort_values(by=['siteId'])\n",
    "all_useful_sites = all_useful_sites.reset_index(drop=True)\n",
    "\n",
    "detector_922 = all_useful_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean the recordes via spatial location - 913 sites\n",
    "The distance between each sensor and its closest road is calculated via ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a list of distance between recorded sites and closest road\n",
    "distance_df = pd.read_csv('/Map matching/closest_road_to_detector_100.csv')\n",
    "distance_df_15 = distance_df.loc[distance_df.NEAR_DIST < 15]\n",
    "\n",
    "detector_922['IN_FID'] = detector_922.index\n",
    "detector_913 = pd.merge(distance_df_15, detector_922, how=\"left\", on=[\"IN_FID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean the dataset via zero percentage - 724 sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the zero percentage of each sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_913['zeroQuantity'] = 0\n",
    "detector_913['zeroPercentage%'] = 0\n",
    "\n",
    "for i in detector_913.siteId.tolist():\n",
    "    print(i)\n",
    "    zero_quantity = 0\n",
    "    site_df = pd.read_csv('data/' + i + '&2019_10_01-2023_09_30.csv')\n",
    "    site_df = site_df.sort_values(by=['lastUpdate']).reset_index(drop=True)\n",
    "    for j in range(len(site_df)):\n",
    "        if site_df.flow[j] == 0:\n",
    "            zero_quantity = zero_quantity + 1\n",
    "    detector_913.loc[detector_913.siteId == i, 'zeroQuantity'] = zero_quantity\n",
    "    detector_913.loc[detector_913.siteId == i, 'zeroPercentage%'] = zero_quantity/len(site_df)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "zeroPercentage% > 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_724 = detector_913.loc[detector_913['zeroPercentage%'] <= 90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clean the dataset via consecutive zero - 470 sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete all zero from original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_zero_flow(data2):\n",
    "    data2 = data2[data2['flow'] != 0]\n",
    "    \n",
    "    data3 = data2.sort_values(by=['lastUpdate'])\n",
    "    data3 = data3.reset_index(drop=True)\n",
    "\n",
    "    return data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the consecutive dates after drop all the zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_check_df(data3, siteId): \n",
    "    \n",
    "    if len(data3) == 0:\n",
    "        zero_check_df = pd.DataFrame({'siteId': siteId,\n",
    "                'start_date': 'none',\n",
    "                'end_date': 'none',\n",
    "                'full_dates': 0,\n",
    "                'practical_dates': 0}, index=[0])\n",
    "        \n",
    "    else:\n",
    "        date_range = data3.iloc[[0, -1]].date.tolist()\n",
    "        start_date = date_range[0]\n",
    "        end_date = date_range[1]\n",
    "        full_dates = pd.DataFrame(pd.date_range(start_date, end_date), columns=['date']) #get the list of dates\n",
    "        #practical_dates = pd.DataFrame({'date':list(set(data3.date))})\n",
    "        zero_check_df = pd.DataFrame({'siteId': siteId,\n",
    "                'start_date': start_date,\n",
    "                'end_date': end_date,\n",
    "                'full_dates': len(full_dates),\n",
    "                'practical_dates': len(set(data3.date))}, index=[0])\n",
    "\n",
    "    return zero_check_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement: Select the sites with the most consecutive flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for i in detector_724.siteId.tolist():\n",
    "    site_df = pd.read_csv('data/'+ i + '&2019_10_01-2023_09_30.csv')\n",
    "    data3 = drop_zero_flow(site_df)\n",
    "    each_check = get_check_df(data3,i)\n",
    "    df_list.append(each_check)\n",
    "    \n",
    "df_check_consecutive = pd.concat(df_list)\n",
    "df_check_consecutive = df_check_consecutive.sort_values(by=['siteId']).reset_index(drop=True)\n",
    "df_check_consecutive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the 470 most useful sites\n",
    "detector_470 = df_check_consecutive.loc[df_check_consecutive.practical_dates == 1411].sort_values(by=['siteId']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get the missing dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips 2: Get the full list of days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_list(start_year, start_month, start_day, end_year, end_month, end_day):\n",
    "    \n",
    "    d0 = date(start_year, start_month, start_day)\n",
    "    d1 = date(end_year, end_month, end_day)\n",
    "    delta = d1 - d0\n",
    "    \n",
    "    full_days = []\n",
    "    for i in range(delta.days):\n",
    "        day = str(d0 + timedelta(days=i))\n",
    "        full_days.append(day)\n",
    "    \n",
    "    return full_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1461"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_days = day_list(2019,10,1,2023,10,1)\n",
    "len(full_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_days(sites_df, path, file_suffix, full_days):\n",
    "    \n",
    "    missing_day_list = []\n",
    "    for i in sites_df.siteId.tolist():\n",
    "        #print(i)\n",
    "        site_df = pd.read_csv(path+ i + file_suffix)\n",
    "        site_df['date'] = site_df['lastUpdate'].str.split('T').str[0]\n",
    "        practical_days = list(site_df.date.unique())\n",
    "        #Get the missing days\n",
    "        missing_days = sorted(list(set(full_days) - set(practical_days)))\n",
    "        missing_day_list.append(missing_days)\n",
    "\n",
    "    #Get frequency of missing days\n",
    "    missing_days = Counter([tuple(sublist) for sublist in missing_day_list])\n",
    "    \n",
    "    return missing_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_days = get_missing_days(detector_470, 'data/', '&2019_10_01-2023_09_30.csv', full_days)\n",
    "missing_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_list = []\n",
    "for i in list(missing_days):\n",
    "    for j in i:\n",
    "        missing_list.append(j)\n",
    "len(missing_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In conclusion, there are 50 missing days of 460 sensors from 2019-10-1 to 2023-9-30\n",
    "\n",
    "Which includes 2019-10-17 to 2019-11-19 (34d); 2022-4-9, 2022-4-10; 2023-4-23, and 2023-7-13 to 2023-7-25 (13d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Aggregate the 470 data on 60-min interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a df with full timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_time_string_list(start,end,interval):\n",
    "    start = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\")\n",
    "    end = datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\")\n",
    "    now = start\n",
    "    string_list = []\n",
    "    while now <= end:\n",
    "        now += timedelta(minutes=15)\n",
    "        string_list.append(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    return string_list\n",
    "\n",
    "time_list = get_time_string_list(\"2019-09-30 23:45:00\",\"2023-09-30 23:30:00\",15)\n",
    "allTime = pd.DataFrame(time_list, columns=['timeStamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeStamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140251</th>\n",
       "      <td>2023-09-30 22:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140252</th>\n",
       "      <td>2023-09-30 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140253</th>\n",
       "      <td>2023-09-30 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140254</th>\n",
       "      <td>2023-09-30 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140255</th>\n",
       "      <td>2023-09-30 23:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140256 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timeStamp\n",
       "0       2019-10-01 00:00:00\n",
       "1       2019-10-01 00:15:00\n",
       "2       2019-10-01 00:30:00\n",
       "3       2019-10-01 00:45:00\n",
       "4       2019-10-01 01:00:00\n",
       "...                     ...\n",
       "140251  2023-09-30 22:45:00\n",
       "140252  2023-09-30 23:00:00\n",
       "140253  2023-09-30 23:15:00\n",
       "140254  2023-09-30 23:30:00\n",
       "140255  2023-09-30 23:45:00\n",
       "\n",
       "[140256 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the traffic flow data to the fullTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the each timestamp to int\n",
    "def recover_lastupdate(data):\n",
    "    data['timeStamp'] = data['lastUpdate'].str.split('.').str[0].str.replace('T',' ')\n",
    "    data['time'] = data['timeStamp'].str.split(' ').str[-1].str.split(':',n=1).str[-1].str.replace(':','.')\n",
    "    data['time'] = pd.to_numeric(data['time'])\n",
    "    for i in range(len(data)):\n",
    "        if 0 <= data['time'][i] < 15:\n",
    "            data.loc[data.index == i,'time'] = 0\n",
    "            #data['time'][i] = 0\n",
    "        elif 15 <= data['time'][i] < 30:\n",
    "            data.loc[data.index == i,'time'] = 15\n",
    "        elif 30 <= data['time'][i] < 45:\n",
    "            data.loc[data.index == i,'time'] = 30\n",
    "        elif 45 <= data['time'][i] < 60:\n",
    "            data.loc[data.index == i,'time'] = 45\n",
    "\n",
    "    data['time'] = data['time'].astype(str).str.replace('.',':').str.zfill(4).str.ljust(5, '0')\n",
    "    data['timeStamp'] = data['timeStamp'].str[:-5] + data['time'].str[-5:]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_allTime(data, allTime):    \n",
    "    # calculat the mean of flow with the same timestamp\n",
    "    data = data.groupby(\"timeStamp\", as_index=False).agg( {'flow':'mean', 'siteSiteid':'first', 'originDescription':'first', \n",
    "                                                           'originLat':'first', 'originLong':'first', 'originEasting': 'first', \n",
    "                                                           'originNorthing':'first'})\n",
    "\n",
    "    # merge the data with full time\n",
    "    fullData = pd.merge(allTime, data, how=\"left\", on=\"timeStamp\")\n",
    "    fullData['siteSiteid'] = data['siteSiteid'][0]\n",
    "    fullData['originDescription'] = data['originDescription'][0]\n",
    "    fullData['originLat'] = data['originLat'][0]\n",
    "    fullData['originLong'] = data['originLong'][0]\n",
    "    fullData['originEasting'] = data['originEasting'][0]\n",
    "    fullData['originNorthing'] = data['originNorthing'][0]\n",
    "    return fullData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate to specifc time interval\n",
    "Aggregate discipline: 2 = 30-min interval, 3 = 45-min interval, 4 = 60-min interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(data, aggregate_len, date):\n",
    "    data = data.groupby(np.arange(len(data))//aggregate_len).agg( {'timeStamp': 'first', 'flow':lambda x: x.mean()*aggregate_len})\n",
    "    data = data.dropna().sort_values(by=['timeStamp']).reset_index(drop=True)\n",
    "    data['flow'] = data['flow'].round(0).astype(int)\n",
    "    \n",
    "    timestamp = data['timeStamp'].str.split(' ', n = 1, expand = True)\n",
    "    data['date'] = timestamp[0]\n",
    "    data['time'] = timestamp[1].str.split(':', n = 1, expand = True)[0].astype(int)\n",
    "    data = data[['date','time','flow']]\n",
    "    \n",
    "    # clip the data from 2019-10-01\n",
    "    start = data.loc[data['date'] == date].index.min()\n",
    "    data = data.iloc[start:].reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement on 470 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detector in detector_470.siteId.tolist():\n",
    "    \n",
    "    data = pd.read_csv('data/' + detector + '&2019_10_01-2023_09_30.csv')    \n",
    "    data = recover_lastupdate(data)\n",
    "    fullData = merge_allTime(data, allTime)\n",
    "    agr_hour = aggregate_data(fullData,4,'2019-10-01')\n",
    "\n",
    "    agr_hour.to_csv('data_470_hourly/' + detector + '.csv', index = False)\n",
    "    \n",
    "data_all = detector_470[['siteId','lat','lon']]\n",
    "data_all = data_all.rename(columns={'siteId': 'id', 'lat': 'latitude', 'lon': 'longitude'})\n",
    "data_all.to_csv('data_470_hourly/locations.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
